{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Juliani's \"Simple Reinforcement Learning with TensorFlow: Q-learning with tables and neural networks\", I refactor it to a JAX implementation.  It mostly consists of a Q-table update based on the Bellman relation, e.g., a global optimum includes a local one.  I wanted to use JAX because it's faster than tensorflow and pytorch, & functionally pure.  However, readability needs work.     \n",
    "\n",
    "References\n",
    "1. \n",
    "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random, tree_util, lax, random\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnv:\n",
    "   # Board encoding:\n",
    "      # 0 -> (S)tart\n",
    "      # 1 -> (F)rozen\n",
    "      # 2 -> (H)ole\n",
    "      # 3 -> (G)oal\n",
    "   def __init__(self,\n",
    "                board = jnp.array([[0, 1, 2],\n",
    "                                   [2, 1, 3]]),\n",
    "                num_rows = 2,\n",
    "                num_cols = 3,\n",
    "                num_states = 6):\n",
    "      self.board = board\n",
    "      # self.board = jnp.array([[0, 1, 2, 2, 2],\n",
    "      #                         [2, 1, 1, 1, 2],\n",
    "      #                         [2, 2, 2, 1, 2],\n",
    "      #                         [1, 2, 2, 1, 3]])\n",
    "      self.num_rows = num_rows\n",
    "      self.num_cols = num_cols\n",
    "      self.num_states = num_states\n",
    "      self.num_actions = 4\n",
    "\n",
    "   def _get_observation(self, state):\n",
    "      return state\n",
    "   \n",
    "   def maybe_reset(self, state, done):\n",
    "      return lax.cond(done, lambda s: 0, lambda s : s, state)\n",
    "\n",
    "   def reset(self):\n",
    "      return 0\n",
    "   \n",
    "   def step(self, state, action):\n",
    "      # Action encoding:\n",
    "      # 0 -> Left\n",
    "      # 1 -> Right\n",
    "      # 2 -> Up\n",
    "      # 3 -> Down\n",
    "      (row, col) = jnp.divmod(state, self.num_cols)\n",
    "      col = lax.cond(action==0, \n",
    "                     lambda c: jnp.maximum(0,c-1),\n",
    "                     lambda c: c,\n",
    "                     col)\n",
    "      col = lax.cond(action==1,\n",
    "                     lambda c: jnp.minimum(c+1, self.num_cols-1),\n",
    "                     lambda c: c,\n",
    "                     col)\n",
    "      row = lax.cond(action==2, \n",
    "                     lambda r: jnp.maximum(0,r-1),\n",
    "                     lambda r: r,\n",
    "                     row)\n",
    "      row = lax.cond(action==3,\n",
    "                     lambda r: jnp.minimum(r+1, self.num_rows-1),\n",
    "                     lambda r: r,\n",
    "                     row)\n",
    "      # S -> 0, F -> 1, H -> 2, G -> 3\n",
    "      start = self.board[row,col] == 0\n",
    "      frozen = self.board[row,col] == 1\n",
    "      hole = self.board[row,col] == 2\n",
    "      goal = self.board[row,col] == 3\n",
    "      reward = start * 0 + frozen * 0 + hole * -1 + goal * 1\n",
    "      state = row*self.num_cols + col\n",
    "      done = jnp.logical_or(goal, hole)\n",
    "      state = self.maybe_reset(state, done)\n",
    "      return state, reward, done\n",
    "\n",
    "   def _tree_flatten(self):\n",
    "      children = ()  # arrays / dynamic values\n",
    "      aux_data = {'board': self.board,\n",
    "                  'num_rows': self.num_rows,\n",
    "                  'num_cols': self.num_cols,\n",
    "                  'num_states': self.num_states}  # static values\n",
    "      return (children, aux_data)\n",
    "\n",
    "   @classmethod\n",
    "   def _tree_unflatten(cls, aux_data, children):\n",
    "      return cls(*children, **aux_data)\n",
    "\n",
    "tree_util.register_pytree_node(MazeEnv,\n",
    "                               MazeEnv._tree_flatten,\n",
    "                               MazeEnv._tree_unflatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [2 1 3]]\n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "[[ 0.41818067  6.3277774   0.43639508 -0.18286254]\n",
      " [ 0.43192235  0.2860769   0.02524948  6.6608186 ]\n",
      " [ 0.71578205  0.6395047   0.12597585  0.23598933]\n",
      " [ 0.9538406   0.40796185  0.99355984  0.5627123 ]\n",
      " [ 0.5185677   7.0113883   0.46324193  0.5417578 ]\n",
      " [ 0.29389143  0.57492447  0.97728324  0.65338516]]\n"
     ]
    }
   ],
   "source": [
    "env = MazeEnv()\n",
    "key = random.PRNGKey(1234)\n",
    "Q = random.uniform(key, (env.num_states, 4))\n",
    "gamma = 0.95\n",
    "alpha = 0.8\n",
    "state = 0\n",
    "num_episodes = 10000\n",
    "def inner_body(i, arguments):\n",
    "   Q, state, env, gamma, alpha, key = arguments\n",
    "   \n",
    "   # action = jnp.argmax(Q[state,:])\n",
    "   action = jnp.argmax(Q[state,:] + random.normal(key,(1,env.num_actions))*(1./(i+1)) )\n",
    "   # action = jnp.argmax(Q[state,:] + random.normal(key,(1,env.num_actions)) )\n",
    "\n",
    "   new_state, reward, done = env.step(state, action)\n",
    "   new_Q_value = Q[state, action] + alpha * (reward + gamma * jnp.max(Q[new_state,:]) - Q[state, action])\n",
    "   Q = Q.at[state, action].set(new_Q_value)\n",
    "   state = new_state\n",
    "   return Q, state, env, gamma, alpha, key\n",
    "def outer_body(i, arguments):\n",
    "   Q, state, env, gamma, alpha, key = arguments\n",
    "   state = env.reset()\n",
    "   Q, state, env, _, _, _ = lax.fori_loop(0, num_episodes, inner_body, (Q, state, env, gamma, alpha, key))\n",
    "   return Q, state, env, gamma, alpha, key\n",
    "Q, _, _, _, _, _ = lax.fori_loop(0, num_episodes, outer_body, (Q, state, env, gamma, alpha, key))\n",
    "print(env.board)\n",
    "print(jnp.reshape(jnp.arange(0,env.num_states),(env.num_rows,env.num_cols)))\n",
    "print(Q)\n",
    "#   Left  Right Up    Down\n",
    "#   0     1     2     3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
