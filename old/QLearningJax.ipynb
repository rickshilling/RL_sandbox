{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, tree_util, random, tree_util, lax\n",
    "import jax.random as random\n",
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnv:\n",
    "   def __init__(self,\n",
    "                board = jnp.array([[0, 1, 2],\n",
    "                                   [2, 1, 3]]),\n",
    "                num_rows = 2,\n",
    "                num_cols = 3,\n",
    "                num_states = 6):\n",
    "      # S -> 0, F -> 1, H -> 2, G -> 3\n",
    "      self.board = board\n",
    "      self.num_rows = num_rows\n",
    "      self.num_cols = num_cols\n",
    "      self.num_states = num_states\n",
    "\n",
    "   def _get_observation(self, state):\n",
    "      return state\n",
    "   \n",
    "   def maybe_reset(self, state, done):\n",
    "      return lax.cond(done, lambda s: 0, lambda s : s, state)\n",
    "\n",
    "   def reset(self):\n",
    "      return 0\n",
    "   \n",
    "   def step(self, state, action):\n",
    "      (row, col) = jnp.divmod(state, self.num_cols)\n",
    "      # Left Right Up Down\n",
    "      # 0    1     2  3\n",
    "      col = lax.cond(action==0, \n",
    "                     lambda c: jnp.maximum(0,c-1),\n",
    "                     lambda c: c,\n",
    "                     col)\n",
    "      col = lax.cond(action==1,\n",
    "                     lambda c: jnp.minimum(c+1, self.num_cols-1),\n",
    "                     lambda c: c,\n",
    "                     col)\n",
    "      row = lax.cond(action==2, \n",
    "                     lambda r: jnp.maximum(0,r-1),\n",
    "                     lambda r: r,\n",
    "                     row)\n",
    "      row = lax.cond(action==3,\n",
    "                     lambda r: jnp.minimum(r+1, self.num_rows-1),\n",
    "                     lambda r: r,\n",
    "                     row)\n",
    "      # S -> 0, F -> 1, H -> 2, G -> 3\n",
    "      start = self.board[row,col] == 0\n",
    "      frozen = self.board[row,col] == 1\n",
    "      hole = self.board[row,col] == 2\n",
    "      goal = self.board[row,col] == 3\n",
    "      reward = start * 0 + frozen * 0 + hole * -1 + goal * 1\n",
    "      state = row*self.num_cols + col\n",
    "      done = jnp.logical_or(goal, hole)\n",
    "      state = self.maybe_reset(state, done)\n",
    "      return state, reward, done\n",
    "\n",
    "   def _tree_flatten(self):\n",
    "      children = ()  # arrays / dynamic values\n",
    "      aux_data = {'board': self.board,\n",
    "                  'num_rows': self.num_rows,\n",
    "                  'num_cols': self.num_cols,\n",
    "                  'num_states': self.num_states}  # static values\n",
    "      return (children, aux_data)\n",
    "\n",
    "   @classmethod\n",
    "   def _tree_unflatten(cls, aux_data, children):\n",
    "      return cls(*children, **aux_data)\n",
    "\n",
    "tree_util.register_pytree_node(MazeEnv,\n",
    "                               MazeEnv._tree_flatten,\n",
    "                               MazeEnv._tree_unflatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_nn(nn.Module):\n",
    "  out_dims: int\n",
    "  @nn.compact\n",
    "  def __call__(self, input):                  # input:                1 x num_states\n",
    "    Qout = nn.Dense(self.out_dims)(input)     # Dense layer: num_states x num_actions\n",
    "    return Qout                               # Qout:                 1 x num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv()\n",
    "model = Q_nn(out_dims=4)\n",
    "state_to_tensor = lambda state: jnp.identity(env.num_states)[state:state+1]\n",
    "tensor_to_state = lambda tensor_state: jnp.argmax(tensor_state)\n",
    "tensor_state = state_to_tensor(0)\n",
    "params = model.init(random.key(42), tensor_state)\n",
    "tx = optax.adam(1e-1)\n",
    "train_state = TrainState.create(\n",
    "    apply_fn = model.apply,\n",
    "    params = params['params'],\n",
    "    tx = tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "# def train_step(train_state, batch):\n",
    "#   def loss_fn(params):\n",
    "#     Qs = train_state.apply_fn({'params': params}, batch['tensor_state'])\n",
    "#     loss = optax.l2_loss(predictions=Qs, labels=batch['Q-target']).mean()\n",
    "#     return loss\n",
    "#   grad_fn = grad(loss_fn)\n",
    "#   grads = grad_fn(train_state.params)\n",
    "#   train_state = train_state.apply_gradients(grads=grads)\n",
    "#   return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_state, batch, env):\n",
    "  def loss_fn(params):\n",
    "    s = tensor_to_state(batch['tensor_state'])\n",
    "    allQ = train_state.apply_fn({'params': params}, batch['tensor_state'])\n",
    "    a = jnp.argmax(allQ)  # Left Right Up Down |-> 0 1 2 3\n",
    "    s1, r, d = env.step(s, a)\n",
    "    s1_tensor = state_to_tensor(s1)\n",
    "    Q1 = train_state.apply_fn({'params': params}, s1_tensor)\n",
    "    maxQ1 = jnp.max(Q1)\n",
    "    gamma = 0.95\n",
    "    alpha = 0.8\n",
    "    targetQ = allQ\n",
    "    targetQ.at[a].set(r + gamma*maxQ1)\n",
    "    loss = optax.l2_loss(predictions=allQ, labels=targetQ).sum()\n",
    "    return loss\n",
    "  grad_fn = grad(loss_fn)\n",
    "  grads = grad_fn(train_state.params)\n",
    "  train_state = train_state.apply_gradients(grads=grads)\n",
    "  return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5\n",
    "gamma = 0.95\n",
    "alpha = 0.8\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    train_state = train_step(train_state)\n",
    "    # j = 0\n",
    "    # while j < 4:\n",
    "    #     j += 1\n",
    "    #     tensor_state = state_to_tensor(state)\n",
    "    #     allQ = model.apply(params, tensor_state)\n",
    "    #     action = jnp.argmax(allQ)  # Left Right Up Down |-> 0 1 2 3\n",
    "    #     new_state, reward, done = env.step(state, action)\n",
    "    #     new_tensor_state = state_to_tensor(new_state)\n",
    "    #     Q1 = model.apply(params, new_tensor_state)\n",
    "    #     maxQ1 = jnp.max(Q1)\n",
    "\n",
    "    #     targetQ = allQ\n",
    "    #     targetQ.at[action].set(reward + gamma*maxQ1 )\n",
    "    #     allQ = model.apply(params, tensor_state)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
