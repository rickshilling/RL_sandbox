{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Juliani's \"Simple Reinforcement Learning with TensorFlow: Q-learning with tables and neural networks\", I refactor it to a JAX implementation.  It mostly consists of a Q-table update based on the Bellman relation, e.g., a global optimum includes a local one.  I wanted to use JAX because it's faster than tensorflow, & functionally pure.  \n",
    "\n",
    "References\n",
    "1. \n",
    "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, tree_util, random, tree_util, lax\n",
    "import jax\n",
    "import jax.random as random\n",
    "from functools import partial\n",
    "from flax.training.train_state import TrainState\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnv:\n",
    "   def __init__(self,\n",
    "                board = jnp.array([[0, 1, 2],\n",
    "                                   [2, 1, 3]]),\n",
    "                num_rows = 2,\n",
    "                num_cols = 3,\n",
    "                num_states = 6):\n",
    "      # S -> 0, F -> 1, H -> 2, G -> 3\n",
    "      # Start   Frozen  Hole    Goal\n",
    "      self.board = board\n",
    "      # self.board = jnp.array([[0, 1, 2, 2, 2],\n",
    "      #                         [2, 1, 1, 1, 2],\n",
    "      #                         [2, 2, 2, 1, 2],\n",
    "      #                         [1, 2, 2, 1, 3]])\n",
    "      self.num_rows = num_rows\n",
    "      self.num_cols = num_cols\n",
    "      self.num_states = num_states\n",
    "\n",
    "   def _get_observation(self, state):\n",
    "      return state\n",
    "   \n",
    "   def maybe_reset(self, state, done):\n",
    "      return lax.cond(done, lambda s: 0, lambda s : s, state)\n",
    "\n",
    "   def reset(self):\n",
    "      return 0\n",
    "   \n",
    "   def step(self, state, action):\n",
    "      (row, col) = jnp.divmod(state, self.num_cols)\n",
    "      # Left Right Up Down\n",
    "      # 0    1     2  3\n",
    "      col = lax.cond(action==0, \n",
    "                     lambda c: jnp.maximum(0,c-1),\n",
    "                     lambda c: c,\n",
    "                     col)\n",
    "      col = lax.cond(action==1,\n",
    "                     lambda c: jnp.minimum(c+1, self.num_cols-1),\n",
    "                     lambda c: c,\n",
    "                     col)\n",
    "      row = lax.cond(action==2, \n",
    "                     lambda r: jnp.maximum(0,r-1),\n",
    "                     lambda r: r,\n",
    "                     row)\n",
    "      row = lax.cond(action==3,\n",
    "                     lambda r: jnp.minimum(r+1, self.num_rows-1),\n",
    "                     lambda r: r,\n",
    "                     row)\n",
    "      # S -> 0, F -> 1, H -> 2, G -> 3\n",
    "      start = self.board[row,col] == 0\n",
    "      frozen = self.board[row,col] == 1\n",
    "      hole = self.board[row,col] == 2\n",
    "      goal = self.board[row,col] == 3\n",
    "      reward = start * 0 + frozen * 0 + hole * -1 + goal * 1\n",
    "      state = row*self.num_cols + col\n",
    "      done = jnp.logical_or(goal, hole)\n",
    "      state = self.maybe_reset(state, done)\n",
    "      return state, reward, done\n",
    "\n",
    "   def _tree_flatten(self):\n",
    "      children = ()  # arrays / dynamic values\n",
    "      aux_data = {'board': self.board,\n",
    "                  'num_rows': self.num_rows,\n",
    "                  'num_cols': self.num_cols,\n",
    "                  'num_states': self.num_states}  # static values\n",
    "      return (children, aux_data)\n",
    "\n",
    "   @classmethod\n",
    "   def _tree_unflatten(cls, aux_data, children):\n",
    "      return cls(*children, **aux_data)\n",
    "\n",
    "tree_util.register_pytree_node(MazeEnv,\n",
    "                               MazeEnv._tree_flatten,\n",
    "                               MazeEnv._tree_unflatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [2 1 3]]\n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "[[ 0.06541302  0.06471601  0.02240455 -0.25877023]\n",
      " [ 0.06506324 -0.70904     0.06424201  0.03644359]\n",
      " [ 0.08744538  0.7909105   0.35205448  0.53364205]\n",
      " [ 0.02900076  0.4168595   0.5802449   0.91486526]\n",
      " [ 0.27414513  0.14991808  0.9383501   0.5209162 ]\n",
      " [ 0.51207185  0.90618336  0.7309413   0.95533276]]\n"
     ]
    }
   ],
   "source": [
    "env = MazeEnv()\n",
    "print(env.board)\n",
    "print(jnp.reshape(jnp.arange(0,env.num_states),(env.num_rows,env.num_cols)))\n",
    "key = random.PRNGKey(0)\n",
    "Q = random.uniform(key, (env.num_states, 4))\n",
    "# print(Q)\n",
    "gamma = 0.95\n",
    "alpha = 0.8\n",
    "num_episodes = 50\n",
    "state = 0\n",
    "for i in range(num_episodes):\n",
    "    if state == (env.num_states-1):\n",
    "        print('Reached goal')\n",
    "        break\n",
    "    # print(i)\n",
    "    state = env.reset()\n",
    "    j = 0\n",
    "    while j < 4:\n",
    "        j += 1\n",
    "        action = jnp.argmax(Q[state,:])\n",
    "        new_state, reward, done = env.step(state, action)\n",
    "        new_Q_value = Q[state, action] + alpha * (reward + gamma * jnp.max(Q[new_state,:]) - Q[state, action])\n",
    "        Q = Q.at[state, action].set(new_Q_value)\n",
    "        state = new_state\n",
    "        if done == True:\n",
    "            break\n",
    "print(Q)\n",
    "#   Left Right Up Down\n",
    "#   0    1     2  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [2 1 3]]\n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "[[ 0.03404112  6.3277774   0.02240455 -0.25877023]\n",
      " [ 0.03598888 -0.70904     0.03627574  6.6608186 ]\n",
      " [ 0.08744538  0.7909105   0.35205448  0.53364205]\n",
      " [ 0.02900076  0.4168595   0.5802449   0.91486526]\n",
      " [-0.71709883  7.0113883   0.14695214  0.14695214]\n",
      " [ 0.51207185  0.90618336  0.7309413   0.95533276]]\n"
     ]
    }
   ],
   "source": [
    "env = MazeEnv()\n",
    "print(env.board)\n",
    "print(jnp.reshape(jnp.arange(0,env.num_states),(env.num_rows,env.num_cols)))\n",
    "key = random.PRNGKey(0)\n",
    "Q = random.uniform(key, (env.num_states, 4))\n",
    "gamma = 0.95\n",
    "alpha = 0.8\n",
    "state = 0\n",
    "num_episodes = 10000\n",
    "def inner_body(i, arguments):\n",
    "   Q, state, env, gamma, alpha = arguments\n",
    "   action = jnp.argmax(Q[state,:])\n",
    "   new_state, reward, done = env.step(state, action)\n",
    "   new_Q_value = Q[state, action] + alpha * (reward + gamma * jnp.max(Q[new_state,:]) - Q[state, action])\n",
    "   Q = Q.at[state, action].set(new_Q_value)\n",
    "   state = new_state\n",
    "   return Q, state, env, gamma, alpha\n",
    "def outer_body(i, arguments):\n",
    "   Q, state, env, gamma, alpha = arguments\n",
    "   state = env.reset()\n",
    "   Q, state, env, _, _ = jax.lax.fori_loop(0, num_episodes, inner_body, (Q, state, env, gamma, alpha))\n",
    "   return Q, state, env, gamma, alpha\n",
    "Q, _, _, _, _ = jax.lax.fori_loop(0, num_episodes, outer_body, (Q, state, env, gamma, alpha))\n",
    "print(Q)\n",
    "#   Left Right Up Down\n",
    "#   0    1     2  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'kernel': Array([[-1.10456   , -1.1868286 ],\n",
      "       [-0.9255007 ,  0.13144489]], dtype=float32), 'bias': Array([0., 0.], dtype=float32)}}\n",
      "3.3514676\n",
      "{'bias': Array([-1.5150304, -1.0276918], dtype=float32), 'kernel': Array([[-1.5150304, -1.0276918],\n",
      "       [-1.5150304, -1.0276918]], dtype=float32)}\n",
      "3.343844\n"
     ]
    }
   ],
   "source": [
    "# import flax.linen as nn\n",
    "# from flax.training.train_state import TrainState\n",
    "# import jax, jax.numpy as jnp\n",
    "# import optax\n",
    "\n",
    "# x = jnp.ones((1,2))\n",
    "# y = jnp.ones((1,2))\n",
    "# model = nn.Dense(2)\n",
    "# variables = model.init(jax.random.key(0), x)\n",
    "# tx = optax.adam(1e-3)\n",
    "\n",
    "# state = TrainState.create(\n",
    "#     apply_fn = model.apply,\n",
    "#     params = variables['params'],\n",
    "#     tx = tx)\n",
    "\n",
    "# print(variables)\n",
    "\n",
    "# def loss_fn(params, x, y):\n",
    "#     predictions = state.apply_fn({'params': params}, x)\n",
    "#     loss = optax.l2_loss(predictions=predictions, targets = y).mean()\n",
    "#     return loss\n",
    "\n",
    "# l = loss_fn(state.params, x, y)\n",
    "# print(l)\n",
    "\n",
    "# grads = jax.grad(loss_fn)(state.params, x, y)\n",
    "# print(grads)\n",
    "\n",
    "# state = state.apply_gradients(grads=grads)\n",
    "# l = loss_fn(state.params, x, y)\n",
    "# print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import jax, jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "# Q-Learning with neural networks (nn)\n",
    "# Since there are more states in a typical game than a normal computer memory can hold we're forced to abandon a table\n",
    "# We create a nn that inputs a hot-vector with length equal to the number of states and outputs a vector of \n",
    "# 4 Q-values -- one for each action.  Instead of updating a table, the nn will update via backpropagation & a loss function.\n",
    "\n",
    "class Q_NN(nn.Module):                    # create a Flax Module dataclass\n",
    "  out_dims: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, input):                  # input:                1 x num_states\n",
    "    Qout = nn.Dense(self.out_dims)(input)     # Dense layer: num_states x num_actions\n",
    "    return Qout                               # Qout:                 1 x num_actions\n",
    "    # action = jnp.argmax(Qout)\n",
    "    # return action\n",
    "    # x = x.reshape((x.shape[0], -1))       # x: 1 x 16\n",
    "    # return x\n",
    "\n",
    "model = Q_NN(out_dims=4)  \n",
    "x = jnp.empty((1, env.num_states))       # generate random data\n",
    "variables = model.init(random.key(42), x)# initialize the weights\n",
    " \n",
    "# print(model.tabulate(jax.random.key(0), x, compute_flops=True, compute_vjp_flops=True))\n",
    "\n",
    "tx = optax.adam(1e-1)\n",
    "state = TrainState.create(\n",
    "    apply_fn = model.apply,\n",
    "    params = variables['params'],\n",
    "    tx = tx)\n",
    "\n",
    "def loss_fn(params, input, nextQ):\n",
    "    Qout = state.apply_fn({'params': params}, input)\n",
    "    prediction = jnp.argmax(Qout)\n",
    "    loss = optax.l2_loss(predictions=prediction, targets = nextQ)\n",
    "    return loss\n",
    "\n",
    "def train_step(state, batch):\n",
    "    grad_fn = jax.grad(loss_fn)\n",
    "    grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv()\n",
    "print(env.board)\n",
    "print(jnp.reshape(jnp.arange(0,env.num_states),(env.num_rows,env.num_cols)))\n",
    "key = random.PRNGKey(0)\n",
    "Q = random.uniform(key, (env.num_states, 4))\n",
    "# print(Q)\n",
    "gamma = 0.95\n",
    "alpha = 0.8\n",
    "num_episodes = 5\n",
    "state = 0\n",
    "for i in range(num_episodes):\n",
    "    if state == (env.num_states-1):\n",
    "        print('Reached goal')\n",
    "        break\n",
    "    # print(i)\n",
    "    state = env.reset()\n",
    "    j = 0\n",
    "    while j < 4:\n",
    "        j += 1\n",
    "        # action = jnp.argmax(Q[state,:])\n",
    "        # new_state, reward, done = env.step(state, action)\n",
    "        # new_Q_value = Q[state, action] + alpha * (reward + gamma * jnp.max(Q[new_state,:]) - Q[state, action])\n",
    "        # Q = Q.at[state, action].set(new_Q_value)\n",
    "        # state = new_state\n",
    "        # if done == True:\n",
    "        #     break\n",
    "print(Q)\n",
    "#   Left Right Up Down\n",
    "#   0    1     2  3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
