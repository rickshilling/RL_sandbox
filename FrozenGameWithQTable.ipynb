{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Juliani's \"Simple Reinforcement Learning with TensorFlow: Q-learning with tables and neural networks\", I refactor it to a JAX implementation.  It mostly consists of a Q-table update based on the Bellman relation, e.g., a global optimum includes a local one.  I wanted to use JAX because it's faster than tensorflow, & functionally pure.  \n",
    "\n",
    "References\n",
    "1. \n",
    "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, tree_util, random, tree_util, lax\n",
    "import jax\n",
    "import jax.random as random\n",
    "from functools import partial\n",
    "from flax.training.train_state import TrainState\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnv:\n",
    "   # Board encoding:\n",
    "      # 0 -> (S)tart\n",
    "      # 1 -> (F)rozen\n",
    "      # 2 -> (H)ole\n",
    "      # 3 -> (G)oal\n",
    "   def __init__(self,\n",
    "                board = jnp.array([[0, 1, 2],\n",
    "                                   [2, 1, 3]]),\n",
    "                num_rows = 2,\n",
    "                num_cols = 3,\n",
    "                num_states = 6):\n",
    "      self.board = board\n",
    "      # self.board = jnp.array([[0, 1, 2, 2, 2],\n",
    "      #                         [2, 1, 1, 1, 2],\n",
    "      #                         [2, 2, 2, 1, 2],\n",
    "      #                         [1, 2, 2, 1, 3]])\n",
    "      self.num_rows = num_rows\n",
    "      self.num_cols = num_cols\n",
    "      self.num_states = num_states\n",
    "\n",
    "   def _get_observation(self, state):\n",
    "      return state\n",
    "   \n",
    "   def maybe_reset(self, state, done):\n",
    "      return lax.cond(done, lambda s: 0, lambda s : s, state)\n",
    "\n",
    "   def reset(self):\n",
    "      return 0\n",
    "   \n",
    "   def step(self, state, action):\n",
    "      # Action encoding:\n",
    "      # 0 -> Left\n",
    "      # 1 -> Right\n",
    "      # 2 -> Up\n",
    "      # 3 -> Down\n",
    "      (row, col) = jnp.divmod(state, self.num_cols)\n",
    "      col = lax.cond(action==0, \n",
    "                     lambda c: jnp.maximum(0,c-1),\n",
    "                     lambda c: c,\n",
    "                     col)\n",
    "      col = lax.cond(action==1,\n",
    "                     lambda c: jnp.minimum(c+1, self.num_cols-1),\n",
    "                     lambda c: c,\n",
    "                     col)\n",
    "      row = lax.cond(action==2, \n",
    "                     lambda r: jnp.maximum(0,r-1),\n",
    "                     lambda r: r,\n",
    "                     row)\n",
    "      row = lax.cond(action==3,\n",
    "                     lambda r: jnp.minimum(r+1, self.num_rows-1),\n",
    "                     lambda r: r,\n",
    "                     row)\n",
    "      # S -> 0, F -> 1, H -> 2, G -> 3\n",
    "      start = self.board[row,col] == 0\n",
    "      frozen = self.board[row,col] == 1\n",
    "      hole = self.board[row,col] == 2\n",
    "      goal = self.board[row,col] == 3\n",
    "      reward = start * 0 + frozen * 0 + hole * -1 + goal * 1\n",
    "      state = row*self.num_cols + col\n",
    "      done = jnp.logical_or(goal, hole)\n",
    "      state = self.maybe_reset(state, done)\n",
    "      return state, reward, done\n",
    "\n",
    "   def _tree_flatten(self):\n",
    "      children = ()  # arrays / dynamic values\n",
    "      aux_data = {'board': self.board,\n",
    "                  'num_rows': self.num_rows,\n",
    "                  'num_cols': self.num_cols,\n",
    "                  'num_states': self.num_states}  # static values\n",
    "      return (children, aux_data)\n",
    "\n",
    "   @classmethod\n",
    "   def _tree_unflatten(cls, aux_data, children):\n",
    "      return cls(*children, **aux_data)\n",
    "\n",
    "tree_util.register_pytree_node(MazeEnv,\n",
    "                               MazeEnv._tree_flatten,\n",
    "                               MazeEnv._tree_unflatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [2 1 3]]\n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "[[ 0.03404112  6.3277774   0.02240455 -0.25877023]\n",
      " [ 0.03598888 -0.70904     0.03627574  6.6608186 ]\n",
      " [ 0.08744538  0.7909105   0.35205448  0.53364205]\n",
      " [ 0.02900076  0.4168595   0.5802449   0.91486526]\n",
      " [-0.71709883  7.0113883   0.14695214  0.14695214]\n",
      " [ 0.51207185  0.90618336  0.7309413   0.95533276]]\n"
     ]
    }
   ],
   "source": [
    "env = MazeEnv()\n",
    "print(env.board)\n",
    "print(jnp.reshape(jnp.arange(0,env.num_states),(env.num_rows,env.num_cols)))\n",
    "key = random.PRNGKey(0)\n",
    "Q = random.uniform(key, (env.num_states, 4))\n",
    "gamma = 0.95\n",
    "alpha = 0.8\n",
    "state = 0\n",
    "num_episodes = 10000\n",
    "def inner_body(i, arguments):\n",
    "   Q, state, env, gamma, alpha = arguments\n",
    "   action = jnp.argmax(Q[state,:])\n",
    "   new_state, reward, done = env.step(state, action)\n",
    "   new_Q_value = Q[state, action] + alpha * (reward + gamma * jnp.max(Q[new_state,:]) - Q[state, action])\n",
    "   Q = Q.at[state, action].set(new_Q_value)\n",
    "   state = new_state\n",
    "   return Q, state, env, gamma, alpha\n",
    "def outer_body(i, arguments):\n",
    "   Q, state, env, gamma, alpha = arguments\n",
    "   state = env.reset()\n",
    "   Q, state, env, _, _ = jax.lax.fori_loop(0, num_episodes, inner_body, (Q, state, env, gamma, alpha))\n",
    "   return Q, state, env, gamma, alpha\n",
    "Q, _, _, _, _ = jax.lax.fori_loop(0, num_episodes, outer_body, (Q, state, env, gamma, alpha))\n",
    "print(Q)\n",
    "#   Left Right Up Down\n",
    "#   0    1     2  3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
